{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris is a famous dataset that has 3 species of iris along with 4 measurements. Learn more here:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "Lots of beginner tutorials use it because the flower types (\"classes\") aren't linearly separatable. This essentially means that different species often have similar measurements, which makes this a good dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we're taking advantage of methods built into this sklearn dataset. In a real-world project you'd be loading a csv or some other data file, and then specifying your x and y columns. It'd look something like:\n",
    "```\n",
    "data = pd.load_csv('myfile.csv')\n",
    "x = data.drop('target_column', axis=1)\n",
    "y = data['target_column')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip the exploratory data analysis (EDA) in this notebook, but you should typically perform it!\n",
    "\n",
    "Next we need to create training and testing sets. Get used to the following code; you'll use it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest =\\\n",
    "train_test_split(x, y, test_size=.2, random_state=10)\n",
    "\n",
    "# Remember that we use random_state for teaching and\n",
    "# reproducibility purposes. You probably won't use it\n",
    "# in your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (120,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 4), (30,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see how good a \"dummy classifier\" does that always predicts the most common $y$ value. Here's how I do this using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43333333333333335"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dum = DummyClassifier()\n",
    "\n",
    "dum.fit(xtrain, ytrain)\n",
    "preds = dum.predict(xtest)\n",
    "\n",
    "accuracy_score(ytest, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those 4 lines of code above are like 80% of machine learning! Get used to those, too.\n",
    "\n",
    "Now let's use a k-nearest neighbors algorithm, which is simple but powerful. The intuition behind KNN is that you should find the data point(s) most similar to the one you're predicting, and make that same prediction.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Let's say I know someone who lives in SF, drives a Prius, is 30 years old, and is not religious. Do you think she's a Democrat or Republican?\n",
    "\n",
    "Well, you'd probably predict she's a Democrat! Because you probably know people in a similar demographic and most (if not all of them) are liberal. That's fundamentally how k-nearest neighbors works -- it asks, \"Is this like something I've seen before?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "preds = clf.predict(xtest)\n",
    "\n",
    "accuracy_score(ytest, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KNN model can predict species with 96.6% accuracy -- much better than our Dummy Classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hyperparameter tuning **\n",
    "\n",
    "We're going to keep it simple for now and test our hyperparameters using a for-loop. The main parameter you care about with KNN is \"k\" -- how many similar data points you should compare the new one to. Should you only look at the one most similar? Or let the top 3 take a vote? Top 5? 9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.9666666666666667],\n",
       " [3, 0.9666666666666667],\n",
       " [5, 0.9666666666666667],\n",
       " [7, 0.9666666666666667],\n",
       " [9, 1.0],\n",
       " [11, 1.0],\n",
       " [13, 1.0],\n",
       " [15, 1.0],\n",
       " [17, 0.9666666666666667],\n",
       " [19, 1.0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for n in range(1, 21, 2):\n",
    "    clf = KNeighborsClassifier(n_neighbors=n)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    preds = clf.predict(xtest)\n",
    "    acc = accuracy_score(ytest, preds)\n",
    "    results.append([n, acc])\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model does really well with 9 neighbors, so you'd probably use that setting in your final model.\n",
    "\n",
    "Note that no real-world model ever does *this* good. It's probably the result of having a tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "preds = clf.predict(xtest)\n",
    "\n",
    "accuracy_score(ytest, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaand there it is again, just so you can see that this is our current model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "The more advanced ML workflow is to split your data into **3** parts: training, testing, and validation.\n",
    "\n",
    "Technically, what we were calling testing data before is actually *validation* data. The main difference is that use validation data to \"check your work\" and tune your model, and testing data is the \"final exam\" to see how good your model actually is.\n",
    "\n",
    "In an ideal scenario, you evaluate the results of your testing data **only once**. It represents real-world data your model has never seen before. And, as such, your model's performance will always be at least slightly worse on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no widely accepted way to code this, so I'll show you my own idiosyncratic way. Feel free to change this to your own style and liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate out the testing data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, test_size=.1, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89285714, 0.96296296, 0.96296296, 1.        , 1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "cvs = cross_val_score(clf, xtrain, ytrain, cv=5, scoring='accuracy')\n",
    "# cv=5 means we're going to run this model 5 times,\n",
    "# each time validating on a different 20%\n",
    "# of the data\n",
    "\n",
    "# cvs is an array of the accuracy scores you obtained:\n",
    "cvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `cvs` itself isn't all that useful to you. What you really want is its mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:      0.9637566137566138\n",
      "Standard deviation: 0.03912840612589273\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy:     ', cvs.mean())\n",
    "print('Standard deviation:', cvs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:      0.9711640211640212\n",
      "Standard deviation: 0.04169835865372268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "cvs = cross_val_score(clf, xtrain, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "print('Mean accuracy:     ', cvs.mean())\n",
    "print('Standard deviation:', cvs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:      0.9783068783068783\n",
      "Standard deviation: 0.02870827506084769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "cvs = cross_val_score(clf, xtrain, ytrain, cv=5, scoring='accuracy')\n",
    "\n",
    "print('Mean accuracy:     ', cvs.mean())\n",
    "print('Standard deviation:', cvs.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 3 we tried above, `n_neighbors=5` looks to be the best.\n",
    "\n",
    "Time for the final exam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you can combine the first 2 lines of code if you prefer!\n",
    "clf = KNeighborsClassifier(n_neighbors=5).fit(xtrain, ytrain)\n",
    "\n",
    "preds = clf.predict(xtest)\n",
    "\n",
    "accuracy_score(ytest, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the basic machine learning workflow. The one major piece you're still missing is how to automate the finding of the best hyperparameters (the knobs you turn -- in this case, just `n_neighbors`.\n",
    "\n",
    "I'll add that to this notebook shortly. Until then, here's another cross validation with a different setting for `n_neighbors`. Hope this helps for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
